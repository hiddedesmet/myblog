<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hidde de Smet</title>
        <description>My blog is a collection of stories, journeys, and ideas for the creatively curious. I share my thoughts on design, technology, with a focus on creativity and innovation.</description>
        <link>http://localhost:4000/</link>
        <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Sun, 25 May 2025 12:28:15 +0200</pubDate>
        <lastBuildDate>Sun, 25 May 2025 12:28:15 +0200</lastBuildDate>
        <generator>Jekyll v4.4.1</generator>
        
            <item>
                <title>A practical guide to Machine Learning for image classification</title>
                <description>&lt;p&gt;I recently started the AI Architecture course by Zach Gardner from IASA Global, which aims to equip professionals with the knowledge to implement AI effectively within businesses. The course delves into AI principles, frameworks, MLOps, governance, and best practices, emphasizing a business-first approach to security, scalability, and performance in AI architectures. Inspired by this, I wanted to share a practical walkthrough of a typical machine learning project.&lt;/p&gt;

&lt;h1 id=&quot;a-practical-guide-to-machine-learning-for-image-classification&quot;&gt;A practical guide to Machine Learning for image classification&lt;/h1&gt;

&lt;p&gt;Many real-world problems involve classifying items based on visual features. Identifying these categories is important for various applications. Often, these classification tasks are performed manually, a process that can be slow and prone to inconsistencies. Machine learning (ML) offers an alternative, enabling computers to learn from examples and automate this process, leading to increased speed, efficiency, and reliability. This post will walk through a common machine learning project focused on image classification, explaining each step from defining the problem to deploying a solution. We’ll see how ML can be used to analyze images and assign them to predefined categories.&lt;/p&gt;

&lt;p&gt;Computers can analyze vast numbers of images quickly without fatigue or distraction. For instance, manually sorting hundreds or thousands of images can lead to errors over time. An ML model, once trained, can maintain consistent performance, ensuring uniform quality in classification tasks.&lt;/p&gt;

&lt;h2 id=&quot;defining-the-problem-image-classification&quot;&gt;Defining the problem: image classification&lt;/h2&gt;

&lt;p&gt;The main challenge in image classification is to analyze an image and determine which predefined category it belongs to. For example, we might need to classify images into:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Object type 1&lt;/li&gt;
  &lt;li&gt;Object type 2&lt;/li&gt;
  &lt;li&gt;Object type 3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each category typically possesses distinct visual characteristics. Differentiating these by eye can be difficult, especially when dealing with a large volume of images or when the visual differences are subtle.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1: Basic image classification process&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;flowchart TD
    A[Input Images] --&amp;gt; B{Classification}
    B --&amp;gt; C[Object type 1]
    B --&amp;gt; D[Object type 2]
    B --&amp;gt; E[Object type 3]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using images for classification is often more efficient than manual inspection. Consider an automated system where items pass by a camera; the camera captures images, and a computer instantly sorts them. This not only saves time but also minimizes errors that might occur due to human fatigue or haste.&lt;/p&gt;

&lt;h2 id=&quot;choosing-the-right-approach-supervised-learning-and-cnns&quot;&gt;Choosing the right approach: supervised learning and CNNs&lt;/h2&gt;

&lt;p&gt;To tackle image classification, we typically turn to &lt;strong&gt;supervised learning&lt;/strong&gt;. In this approach, we provide the computer with a large dataset of examples where the correct answer (the category label) is already known. The model learns to recognize patterns from these labeled examples.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2: Supervised learning with CNNs&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;graph LR
    Input[Input: Labeled images] --&amp;gt; Model[Convolutional Neural Network]
    Model --&amp;gt; Output[Output: Category label]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Supervised learning with CNNs is like teaching a child with flashcards: “This image is object type 1,” “This one is object type 2,” and so on. CNNs are effective because they can automatically learn hierarchical features from images, such as edges, textures, and complex shapes, which are important for accurate classification.&lt;/p&gt;

&lt;h2 id=&quot;essential-tools-for-the-workflow&quot;&gt;Essential tools for the workflow&lt;/h2&gt;

&lt;p&gt;A machine learning project relies on a set of tools to manage the various stages of development. Here are some common categories and examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;ML frameworks&lt;/strong&gt;: These provide the building blocks for creating and training models.
    &lt;ul&gt;
      &lt;li&gt;TensorFlow (often with Keras API)&lt;/li&gt;
      &lt;li&gt;PyTorch&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data labeling tools&lt;/strong&gt;: Used to annotate images with their correct categories.
    &lt;ul&gt;
      &lt;li&gt;LabelImg&lt;/li&gt;
      &lt;li&gt;Roboflow&lt;/li&gt;
      &lt;li&gt;CVAT (Computer Vision Annotation Tool)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Experiment tracking&lt;/strong&gt;: Helps monitor and compare different model versions and training runs.
    &lt;ul&gt;
      &lt;li&gt;MLflow&lt;/li&gt;
      &lt;li&gt;TensorBoard (especially for TensorFlow)&lt;/li&gt;
      &lt;li&gt;Weights &amp;amp; Biases&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The typical workflow involving these tools can be visualized as follows:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3: Data preparation workflow&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;flowchart LR
    A[Data collection] --&amp;gt; B[Labeling tool]
    B --&amp;gt; C[ML framework]
    C --&amp;gt; D[Experiment tracking]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First, we collect the necessary images. Then, using a labeling tool, we assign the correct category to each image. With the labeled dataset, we use an ML framework like TensorFlow or PyTorch to design and train our CNN model. Throughout this process, experiment tracking tools log metrics, parameters, and artifacts, allowing us to reproduce results and understand what works best. These tools are like a scientist’s lab notebook, helpful for systematic improvement.&lt;/p&gt;

&lt;h2 id=&quot;preparing-the-data-collection-splitting-and-augmentation&quot;&gt;Preparing the data: collection, splitting, and augmentation&lt;/h2&gt;

&lt;p&gt;The quality and quantity of data are very important in machine learning. For our image classification model to learn effectively, it needs to see a diverse set of examples.&lt;/p&gt;

&lt;p&gt;Key steps in data preparation include:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Collect diverse, labeled images&lt;/strong&gt;: Gather a wide variety of images for each category, ensuring they represent different conditions (lighting, angles, backgrounds) the model might encounter in the real world.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Split data&lt;/strong&gt;: Divide the dataset into three distinct subsets:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Training set (e.g., 70%)&lt;/strong&gt;: Used to train the model.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Validation set (e.g., 15%)&lt;/strong&gt;: Used to tune model parameters and monitor for overfitting during training.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Test set (e.g., 15%)&lt;/strong&gt;: Used for a final, unbiased evaluation of the trained model’s performance on unseen data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Use data augmentation&lt;/strong&gt;: Artificially increase the size and diversity of the training set by applying random transformations to existing images (e.g., rotations, flips, brightness adjustments). This helps the model become more robust and generalize better to new, unseen images.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here’s an example of how you can set up data augmentation using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ImageDataGenerator&lt;/code&gt; in TensorFlow/Keras:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorflow.keras.preprocessing.image&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ImageDataGenerator&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create an ImageDataGenerator instance with desired augmentations
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datagen&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ImageDataGenerator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rotation_range&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# Randomly rotate images by up to 20 degrees
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;width_shift_range&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Randomly shift images horizontally by up to 20% of the width
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;height_shift_range&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Randomly shift images vertically by up to 20% of the height
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;shear_range&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# Apply shear transformations
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;zoom_range&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;         &lt;span class=&quot;c1&quot;&gt;# Randomly zoom into images
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;horizontal_flip&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# Randomly flip images horizontally
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;fill_mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;nearest&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;     &lt;span class=&quot;c1&quot;&gt;# Strategy for filling newly created pixels
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Example: Applying it to a training data generator
# train_generator = datagen.flow_from_directory(
#     &apos;path/to/train_data&apos;,
#     target_size=(224, 224),
#     batch_size=32,
#     class_mode=&apos;categorical&apos;
# )
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 3: Data preparation workflow&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;flowchart TD
    A[Raw Images] --&amp;gt; B[Labeling]
    B --&amp;gt; C[Dataset split]
    C --&amp;gt; D1[Training set]
    C --&amp;gt; D2[Validation set]
    C --&amp;gt; D3[Test set]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Splitting the data is important to ensure the model isn’t just “memorizing” the training examples but is actually learning to generalize. Data augmentation acts as a regularizer, preventing the model from becoming too specialized to the training data and improving its performance on real-world data.&lt;/p&gt;

&lt;h2 id=&quot;building-and-training-the-model&quot;&gt;Building and training the model&lt;/h2&gt;

&lt;p&gt;With the data prepared, the next step is to define the model architecture and train it.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Choose a CNN architecture&lt;/strong&gt;: Select a CNN architecture suitable for image classification. This could be a custom-built network or a pre-trained model using &lt;strong&gt;transfer learning&lt;/strong&gt;. Transfer learning is a powerful technique where a model developed for a task (e.g., classifying a large dataset like ImageNet) is reused as the starting point for a model on a second task. This approach can significantly reduce training time and improve performance, especially when your dataset is relatively small, as the model has already learned general features from the larger dataset.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Example architecture&lt;/strong&gt;: A simple CNN might consist of:
    &lt;ul&gt;
      &lt;li&gt;Input layer (receiving image data)&lt;/li&gt;
      &lt;li&gt;Convolutional layers (Conv2D) with activation functions (e.g., ReLU)&lt;/li&gt;
      &lt;li&gt;Pooling layers (MaxPooling) to reduce dimensionality&lt;/li&gt;
      &lt;li&gt;Flatten layer (to convert 2D feature maps to a 1D vector)&lt;/li&gt;
      &lt;li&gt;Dense layers (fully connected layers) for classification&lt;/li&gt;
      &lt;li&gt;Output layer with an activation function (e.g., softmax for multi-class classification)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Compile the model&lt;/strong&gt;: Configure the learning process by specifying:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Optimizer&lt;/strong&gt; (e.g., Adam, SGD): Algorithm to update model weights.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Loss function&lt;/strong&gt; (e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;categorical_crossentropy&lt;/code&gt; for multi-class): Measures how well the model is performing.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Metrics&lt;/strong&gt; (e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;accuracy&lt;/code&gt;): Used to monitor training and testing steps.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Train the model&lt;/strong&gt;: Fit the model to the training data, using the validation set to monitor its performance and prevent overfitting.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here&apos;s a simplified example of defining and compiling a CNN model using TensorFlow/Keras:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorflow.keras&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Assuming 3 categories and input images of size 224x224x3 (RGB)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MaxPooling2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MaxPooling2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Output layer for 3 classes
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;adam&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;categorical_crossentropy&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# model.fit(training_data, validation_data=validation_data, epochs=N) # Actual training step
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The model&apos;s architecture dictates its capacity to learn. Convolutional layers act as feature extractors, learning to identify patterns like edges and textures. Pooling layers help to make the learned features more robust to variations in object scale and position. Dense layers then use these high-level features to make the final classification. The training process iteratively adjusts the model&apos;s weights to minimize the chosen loss function.&lt;/p&gt;

&lt;h2 id=&quot;saving-your-trained-model&quot;&gt;Saving your trained model&lt;/h2&gt;

&lt;p&gt;Once the model is trained to a satisfactory performance level, it&apos;s important to save its learned parameters (weights) and architecture. This allows you to reuse the model later for predictions without needing to retrain it from scratch.&lt;/p&gt;

&lt;p&gt;In TensorFlow/Keras, saving a model is straightforward:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Assume &apos;model&apos; is your trained Keras model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;image_classifier_model&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This command saves the entire model (architecture, weights, and training configuration) to a directory named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;image_classifier_model&lt;/code&gt;. This saved model can then be loaded into other applications or deployed to a server. It’s like saving your progress in a complex task, ensuring your efforts are preserved for future use.&lt;/p&gt;

&lt;h2 id=&quot;making-the-model-accessible-serving-with-flask&quot;&gt;Making the model accessible: serving with Flask&lt;/h2&gt;

&lt;p&gt;To make your trained image classification model usable by other applications or users, you can expose it as a web API. Flask is a lightweight Python web framework that is excellent for this purpose.&lt;/p&gt;

&lt;p&gt;Here’s a conceptual example of a Flask app that loads the saved TensorFlow model and provides a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/predict&lt;/code&gt; endpoint:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flask&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jsonify&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PIL&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Pillow library for image manipulation
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Flask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Load the saved model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;load_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;image_classifier_model&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Define the class names (ensure order matches model output)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CLASSES&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Object type 1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Object type 2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Object type 3&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;preprocess_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;convert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;RGB&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Ensure 3 channels
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Resize to model&apos;s expected input size
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;img_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;255.0&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Normalize pixel values
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;img_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;expand_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Add batch dimension
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_array&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@app.route&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/predict&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;methods&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;POST&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;jsonify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;No file part&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;jsonify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;No selected file&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;preprocess_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;class_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;jsonify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;class&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CLASSES&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;confidence&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])})&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;jsonify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;__main__&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0.0.0.0&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This Flask application creates an endpoint that accepts an image file, preprocesses it to match the model’s input requirements, gets a prediction from the loaded TensorFlow model, and returns the predicted class as a JSON response. This makes the model accessible over the network.&lt;/p&gt;

&lt;h2 id=&quot;ensuring-portability-dockerizing-the-application&quot;&gt;Ensuring portability: dockerizing the application&lt;/h2&gt;

&lt;p&gt;To ensure that your Flask application (and the ML model it serves) runs consistently across different environments (development, testing, production), containerization with Docker is highly recommended. Docker packages the application and all its dependencies into a standardized unit called a container.&lt;/p&gt;

&lt;p&gt;Here’s an example &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt; for the Flask application:&lt;/p&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Use an official Python runtime as a parent image&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; python:3.10-slim&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Set the working directory in the container&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /app&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Copy the current directory contents into the container at /app&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; . /app&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Copy requirements.txt and install dependencies&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; requirements.txt .&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--no-cache-dir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; requirements.txt

&lt;span class=&quot;c&quot;&gt;# Make port 5000 available to the world outside this container&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;EXPOSE&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; 5000&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Define environment variable&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; NAME World&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Run app.py when the container launches&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CMD&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; [&quot;python&quot;, &quot;app.py&quot;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You would create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt; file in the same directory as your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app.py&lt;/code&gt;. For this project, it would look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;flask
tensorflow
pillow
numpy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt; defines the steps to build a Docker image. It starts from a base Python image, copies the application code (including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app.py&lt;/code&gt;, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;image_classifier_model&lt;/code&gt; directory, and a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt; file), installs dependencies, exposes the port Flask is running on, and specifies the command to run the application. This container can then be deployed on any system with Docker installed, resolving the “it works on my machine” problem.&lt;/p&gt;

&lt;h2 id=&quot;the-complete-workflow&quot;&gt;The complete workflow&lt;/h2&gt;

&lt;p&gt;The overall workflow, from a user or system providing an image to receiving a classification, can be summarized with the following diagram:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4: Complete image classification and serving workflow&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;flowchart TD
    A[&quot;User Uploads Image / Image from System&quot;] --&amp;gt; B[&quot;Flask API (via HTTP)&quot;]
    B --&amp;gt; C[&quot;Docker Container hosting Flask App &amp;amp; TensorFlow Model&quot;]
    C -- Preprocesses Image --&amp;gt; D[TensorFlow Model Inference]
    D -- Returns Prediction --&amp;gt; C
    C -- Sends JSON Response --&amp;gt; A[&quot;Prediction (Category) returned to User/System&quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A user or an automated system sends an image to the Flask API. The API, running inside a Docker container, receives the image. The Flask application preprocesses the image and feeds it to the TensorFlow model for inference. The model returns a prediction, which the Flask app then formats as a JSON response and sends back to the requester.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-and-key-takeaways&quot;&gt;Conclusion and key takeaways&lt;/h2&gt;

&lt;p&gt;This post highlighted a common and effective machine learning workflow for image classification. The key stages include:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Problem definition&lt;/strong&gt;: Clearly understanding the classification task.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data management&lt;/strong&gt;: Collecting, labeling, splitting, and augmenting image data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model development&lt;/strong&gt;: Choosing an appropriate architecture (like a CNN), training it with frameworks such as TensorFlow, and saving the trained model.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Serving the model via a web API using Flask.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Packaging&lt;/strong&gt;: Containerizing the application with Docker for portability and scalability.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This structured approach can be adapted for a wide array of applications, from identifying different types of flora and fauna to detecting defects in manufacturing or recognizing landmarks in photographs. By following these steps and leveraging the right tools, you can build AI systems capable of understanding and interpreting visual information.&lt;/p&gt;
</description>
                <pubDate>Wed, 21 May 2025 09:00:00 +0200</pubDate>
                <link>http://localhost:4000/iasa-ai-course</link>
                <guid isPermaLink="true">http://localhost:4000/iasa-ai-course</guid>
                
                <category>AI</category>
                
                <category>Machine Learning</category>
                
                <category>Image Classification</category>
                
                <category>IASA</category>
                
                
            </item>
        
            <item>
                <title>Understanding the Model Context Protocol (MCP)</title>
                <description>&lt;p&gt;The Model Context Protocol (MCP) is revolutionizing the way AI models interact with external data and tools. Developed as an open-source standard, MCP simplifies integration by providing a universal connector that eliminates the need for custom-built solutions. This protocol is not just a tool for developers but a gateway to unlocking the full potential of AI applications.&lt;/p&gt;

&lt;h2 id=&quot;what-is-mcp&quot;&gt;What is MCP?&lt;/h2&gt;

&lt;p&gt;MCP is a client-server architecture supported by JSON-RPC 2.0, ensuring secure and efficient communication. It allows AI models to connect to external systems like Google Drive, GitHub, or Slack, enabling them to read, process, and act on data in a context-aware manner. For example, the Claude desktop app acts as an MCP client, requesting data from an MCP server that provides the necessary context.&lt;/p&gt;

&lt;h2 id=&quot;key-features-of-mcp&quot;&gt;Key features of MCP&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Standardization&lt;/strong&gt;: MCP offers a unified protocol for AI integration, reducing complexity.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;: It supports diverse use cases, from database queries to API integrations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: Ensures secure data exchange between AI models and external systems.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Designed to handle growing demands in AI applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-mcp-works&quot;&gt;How MCP works&lt;/h2&gt;

&lt;p&gt;MCP operates on a two-way connection:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;MCP client&lt;/strong&gt;: Requests data or actions from the server.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;MCP server&lt;/strong&gt;: Provides the requested data or executes actions based on the client’s needs.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This architecture enables seamless communication and enhances the responsiveness of AI models.&lt;/p&gt;

&lt;h2 id=&quot;use-cases&quot;&gt;Use cases&lt;/h2&gt;

&lt;p&gt;MCP is already being adopted by leading companies like Microsoft, Google, and OpenAI. Its applications include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Knowledge graph management&lt;/strong&gt;: Streamlining data organization and retrieval.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;API integrations&lt;/strong&gt;: Simplifying connections between AI models and external APIs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tool interactions&lt;/strong&gt;: Enabling AI to interact with tools like Slack or GitHub.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-future-of-mcp&quot;&gt;The future of MCP&lt;/h2&gt;

&lt;p&gt;As we move into an era of agentic AI, MCP is set to play a pivotal role in making AI assistants more versatile and powerful. By breaking down data silos and enhancing integration capabilities, MCP is paving the way for more intelligent and responsive AI systems.&lt;/p&gt;

&lt;p&gt;Would you like to explore how MCP can transform your AI workflows? Let me know in the comments below!&lt;/p&gt;
</description>
                <pubDate>Thu, 08 May 2025 09:00:00 +0200</pubDate>
                <link>http://localhost:4000/model-context-protocol-mcp</link>
                <guid isPermaLink="true">http://localhost:4000/model-context-protocol-mcp</guid>
                
                <category>AI</category>
                
                <category>MCP</category>
                
                
            </item>
        
            <item>
                <title>GitHub Copilot Agent Mode - Transforming your development workflow</title>
                <description>&lt;p&gt;GitHub Copilot Agent Mode takes pair programming to the next level by enabling natural conversations about your code directly in your IDE. This powerful feature transforms the traditional code completion experience into an interactive dialogue that helps you solve problems, understand concepts, and write better code.&lt;/p&gt;

&lt;h2 id=&quot;what-is-agent-mode&quot;&gt;What is Agent Mode?&lt;/h2&gt;

&lt;p&gt;Agent Mode elevates GitHub Copilot from a code completion tool to an interactive AI programming assistant. It allows developers to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Have natural conversations about code and development tasks&lt;/li&gt;
  &lt;li&gt;Get contextual explanations and suggestions&lt;/li&gt;
  &lt;li&gt;Receive step-by-step guidance for complex implementations&lt;/li&gt;
  &lt;li&gt;Debug code through interactive dialogue&lt;/li&gt;
  &lt;li&gt;Learn about best practices and patterns while coding&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-features&quot;&gt;Key features&lt;/h2&gt;

&lt;h3 id=&quot;natural-language-interaction&quot;&gt;Natural language interaction&lt;/h3&gt;
&lt;p&gt;Instead of just suggesting code completions, Agent Mode understands and responds to questions, explains concepts, and helps solve problems through natural conversation. This makes it easier to explore solutions and understand the reasoning behind code suggestions.&lt;/p&gt;

&lt;h3 id=&quot;context-aware-assistance&quot;&gt;Context-aware assistance&lt;/h3&gt;
&lt;p&gt;Agent Mode maintains context throughout your coding session, understanding:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Your project structure and dependencies&lt;/li&gt;
  &lt;li&gt;Previous conversations and decisions&lt;/li&gt;
  &lt;li&gt;Code patterns and conventions you’re using&lt;/li&gt;
  &lt;li&gt;The specific problem you’re trying to solve&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intelligent-problem-solving&quot;&gt;Intelligent problem solving&lt;/h3&gt;
&lt;p&gt;When faced with a programming challenge, Agent Mode can:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Break down complex problems into manageable steps&lt;/li&gt;
  &lt;li&gt;Suggest multiple approaches with pros and cons&lt;/li&gt;
  &lt;li&gt;Help debug issues by analyzing error messages&lt;/li&gt;
  &lt;li&gt;Recommend optimizations and improvements&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;learning-and-documentation&quot;&gt;Learning and documentation&lt;/h3&gt;
&lt;p&gt;Agent Mode serves as an interactive learning tool by:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Explaining code concepts in detail&lt;/li&gt;
  &lt;li&gt;Providing relevant documentation and examples&lt;/li&gt;
  &lt;li&gt;Suggesting best practices and patterns&lt;/li&gt;
  &lt;li&gt;Offering alternative approaches to problems&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;best-practices-for-using-agent-mode&quot;&gt;Best practices for using Agent Mode&lt;/h2&gt;

&lt;p&gt;To get the most out of GitHub Copilot Agent Mode:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Be specific&lt;/strong&gt;: While Agent Mode understands natural language, being specific about your requirements helps get better results.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Iterate through solutions&lt;/strong&gt;: Use the interactive nature to explore different approaches and understand trade-offs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ask for explanations&lt;/strong&gt;: Don’t just accept suggestions; ask why certain approaches are recommended.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Leverage context&lt;/strong&gt;: Let Agent Mode know about your project’s constraints and requirements.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;real-world-applications&quot;&gt;Real-world applications&lt;/h2&gt;

&lt;p&gt;Agent Mode shines in various development scenarios:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Complex problem solving&lt;/strong&gt;: Breaking down and implementing difficult algorithms&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Code refactoring&lt;/strong&gt;: Getting guidance on improving code structure&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Learning new technologies&lt;/strong&gt;: Understanding unfamiliar frameworks or libraries&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Debugging&lt;/strong&gt;: Interactive troubleshooting of issues&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Code review&lt;/strong&gt;: Getting feedback on code quality and potential improvements&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-future-of-ai-pair-programming&quot;&gt;The future of AI pair programming&lt;/h2&gt;

&lt;p&gt;As Agent Mode continues to evolve, we can expect:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Even more natural and context-aware interactions&lt;/li&gt;
  &lt;li&gt;Better understanding of project-specific patterns&lt;/li&gt;
  &lt;li&gt;Enhanced integration with development workflows&lt;/li&gt;
  &lt;li&gt;Improved learning and documentation capabilities&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GitHub Copilot Agent Mode takes pair programming to the next level, making programming more accessible, efficient, and educational. Whether you’re a seasoned developer or just starting, Agent Mode provides valuable assistance that adapts to your needs and helps you write better code.&lt;/p&gt;

&lt;p&gt;Have you tried GitHub Copilot Agent Mode? Share your experiences in the comments below!&lt;/p&gt;
</description>
                <pubDate>Wed, 07 May 2025 09:00:00 +0200</pubDate>
                <link>http://localhost:4000/github-copilot-agent-mode</link>
                <guid isPermaLink="true">http://localhost:4000/github-copilot-agent-mode</guid>
                
                <category>AI</category>
                
                <category>Development</category>
                
                
            </item>
        
    </channel>
</rss>